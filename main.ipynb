{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "## Istanbul Technical University\n",
    "### Computer Engineering Department\n",
    "### BLG 527E - Deep Learning\n",
    "### Spring 2023\n",
    "\n",
    "<b>Student Name: Muhammed Rüşen Birben</b><br>\n",
    "<b>Student ID: 150220755</b><br>\n",
    "<b>Student Email: birben20@itu.edu.tr</b><br>\n",
    "\n",
    "<b>Student Name: Ahmed Burak Ercan</b><br>\n",
    "<b>Student ID: 150220749</b><br>\n",
    "<b>Student Email: ercana20@itu.edu.tr</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'importlib.reload(get_model)\\nimportlib.reload(model_predict)\\nimportlib.reload(get_label_output)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from os import makedirs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from utils import get_model, model_predict, get_label_output\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import importlib\n",
    "# To reload the module after modifications\n",
    "\"\"\"importlib.reload(get_model)\n",
    "importlib.reload(model_predict)\n",
    "importlib.reload(get_label_output)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsAI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Have you ever heard of the Crusades? A time in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLM</td>\n",
       "      <td>The professors, who likely have nearly a decad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Kemba Walker does a good job of defending Foye...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM</td>\n",
       "      <td>Ganias' lawyer, Stanley Twardy, urged the gove...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLM</td>\n",
       "      <td>The Circuit Court of Appeals of New Jersey had...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source                                               Text  IsAI\n",
       "0    LLM  Have you ever heard of the Crusades? A time in...     1\n",
       "1    LLM  The professors, who likely have nearly a decad...     1\n",
       "2    LLM  Kemba Walker does a good job of defending Foye...     1\n",
       "3    LLM  Ganias' lawyer, Stanley Twardy, urged the gove...     1\n",
       "4    LLM  The Circuit Court of Appeals of New Jersey had...     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data.\n",
    "df = pd.read_csv('datasetV2.csv')\n",
    "df.head()\n",
    "#df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df[\"IsAI\"] == 0][:500]\n",
    "df_filtered = pd.concat([df_filtered, df[df[\"IsAI\"] == 1][:500]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.Text\n",
    "y = df_filtered.IsAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_name, X):\n",
    "    # load model\n",
    "    model, tokenizer = get_model(model_name)\n",
    "\n",
    "    # sentences\n",
    "    text = X.values.tolist()\n",
    "\n",
    "    # predict\n",
    "    predictions = model_predict(model, tokenizer, text)\n",
    "\n",
    "    return predictions, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(y_predictons, y_true):\n",
    "    #Evaluate the model by computing precision, recall and F1-score.\n",
    "    pred_labels = get_label_output(y_predictons)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, pred_labels)\n",
    "    recall = recall_score(y_true, pred_labels, average='macro')\n",
    "    precision = precision_score(y_true, pred_labels, average='macro')\n",
    "    f1 = f1_score(y_true, pred_labels, average='macro')\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true, pred_labels)\n",
    "    \n",
    "    print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "    print('Recall: {:.2f}%'.format(recall*100))\n",
    "    print('Precision: {:.2f}%'.format(precision*100))\n",
    "    print('F1-score: {:.2f}%'.format(f1*100))\n",
    "\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 10/10 [02:17<00:00, 13.80s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions, model, tokenizer = predict(\"chatgpt-detector-lli-hc3\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.90%\n",
      "Recall: 53.90%\n",
      "Precision: 54.36%\n",
      "F1-score: 52.64%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[188, 312],\n",
       "       [149, 351]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the DataFrame into train and remaining datasets\n",
    "train_df, remaining_df = train_test_split(df, test_size=0.30, random_state=42)\n",
    "\n",
    "# Splitting the remaining dataset into validation and test datasets\n",
    "val_df, test_df = train_test_split(remaining_df, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(train_df['Text'].tolist(), padding=True, truncation=True, max_length=512)\n",
    "val_encodings = tokenizer(val_df['Text'].tolist(), padding=True, truncation=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['Text'].tolist(), padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_df['IsAI'].tolist())\n",
    "val_dataset = TextDataset(val_encodings, val_df['IsAI'].tolist())\n",
    "test_dataset = TextDataset(test_encodings, test_df['IsAI'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cafdc7e5ed6430d9b7e8c51338de39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd732e13924340cd82b80532b3f4b396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3414738178253174, 'eval_runtime': 2.2329, 'eval_samples_per_second': 8.061, 'eval_steps_per_second': 1.344, 'epoch': 1.0}\n",
      "{'loss': 1.5254, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913e3797c637440cbf4ab78a52903376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.273801326751709, 'eval_runtime': 2.2422, 'eval_samples_per_second': 8.028, 'eval_steps_per_second': 1.338, 'epoch': 2.0}\n",
      "{'loss': 1.0252, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b89716e364e4067ae14249442bef6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1937499046325684, 'eval_runtime': 2.269, 'eval_samples_per_second': 7.933, 'eval_steps_per_second': 1.322, 'epoch': 3.0}\n",
      "{'train_runtime': 117.3427, 'train_samples_per_second': 1.79, 'train_steps_per_second': 0.23, 'train_loss': 1.1908508936564128, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27, training_loss=1.1908508936564128, metrics={'train_runtime': 117.3427, 'train_samples_per_second': 1.79, 'train_steps_per_second': 0.23, 'train_loss': 1.1908508936564128, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"      # evaluate each `logging_steps`\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bb23de9cc54e728547da6caccb012c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "predictons = F.softmax(torch.from_numpy(predictions.predictions), dim=1)\n",
    "predictons = predictons.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax label\n",
    "pred_labels = get_label_output(predictons)\n",
    "y = test_df['IsAI'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.67%\n",
      "Recall: 37.50%\n",
      "Precision: 38.57%\n",
      "F1-score: 37.78%\n"
     ]
    }
   ],
   "source": [
    "#eval_model(pred_labels, y) # y düzenle\n",
    "\n",
    "accuracy = accuracy_score(y, pred_labels)\n",
    "recall = recall_score(y, pred_labels, average='macro')\n",
    "precision = precision_score(y, pred_labels, average='macro')\n",
    "f1 = f1_score(y, pred_labels, average='macro')\n",
    "\n",
    "conf_matrix = confusion_matrix(y, pred_labels)\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(accuracy*100))\n",
    "print('Recall: {:.2f}%'.format(recall*100))\n",
    "print('Precision: {:.2f}%'.format(precision*100))\n",
    "print('F1-score: {:.2f}%'.format(f1*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
